{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrrroSxKuILw7cwPc9sbS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaayaoli/An-lisis-de-datos-conversacionales/blob/main/Notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 游늷 Secci칩n 1: Importar librer칤as y cargar datos\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar archivo Excel con los datos de intenciones\n",
        "df = pd.read_excel(\"../data/IA.xlsx\")\n",
        "\n",
        "# Vista previa de las primeras filas\n",
        "df.head()\n",
        "\n",
        "# ---\n",
        "# 游늷 Secci칩n 2: Filtrar datos\n",
        "# Nos quedamos solo con las filas donde el intent es el especificado\n",
        "df_filtrado = df[df[\"Nombre de Intent\"] == \"0.0. Enviar mensaje a LLM Default\"]\n",
        "\n",
        "print(\"N칰mero de filas filtradas:\", df_filtrado.shape)\n",
        "df_filtrado.head()\n",
        "\n",
        "# Asumimos que la columna de texto se llama \"Texto\" (ajustar si cambia)\n",
        "textos_entrada = df_filtrado[\"Texto\"].dropna().tolist()\n",
        "\n",
        "# ---\n",
        "# 游늷 Secci칩n 3: Preprocesamiento con spaCy\n",
        "!pip install -q spacy\n",
        "!python -m spacy download es_core_news_sm -q\n",
        "\n",
        "import spacy\n",
        "import string\n",
        "\n",
        "# Cargar modelo en espa침ol\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Funci칩n de limpieza y lematizaci칩n\n",
        "def limpiar_y_lemmatizar(texto):\n",
        "    doc = nlp(texto.lower())  # pasar a min칰sculas y procesar con spaCy\n",
        "    tokens_limpios = [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.is_alpha and not token.is_stop\n",
        "    ]\n",
        "    return tokens_limpios\n",
        "\n",
        "# Aplicamos la funci칩n a todos los textos\n",
        "textos_procesados = [limpiar_y_lemmatizar(texto) for texto in textos_entrada]\n",
        "\n",
        "# ---\n",
        "# 游늷 Secci칩n 4: Filtrar palabras irrelevantes\n",
        "palabras_excluir = {\n",
        "    \"event\", \"detection\", \"eh\", \"querer\", \"salir\", \"llegar\", \"q\", \"ok\", \"favor\",\n",
        "    \"bienvenida\", \"ayudar\", \"necesitar\", \"gracias\", \"pasar\", \"ayer\",\n",
        "    \"esperar\", \"dejar\", \"hola\", \"dar\", \"bastante\", \"hacer\"\n",
        "}\n",
        "\n",
        "textos_filtrados = [\n",
        "    [token for token in texto if token not in palabras_excluir]\n",
        "    for texto in textos_procesados\n",
        "]\n",
        "\n",
        "print(\"Texto original:\", textos_entrada[0])\n",
        "print(\"Texto procesado:\", textos_filtrados[0])\n",
        "\n",
        "# ---\n",
        "# 游늷 Secci칩n 5: Frecuencia de palabras\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "\n",
        "# Aplanar todos los tokens\n",
        "tokens_totales = list(chain.from_iterable(textos_filtrados))\n",
        "\n",
        "# Contar palabras\n",
        "frecuencia_palabras = Counter(tokens_totales)\n",
        "\n",
        "# Mostrar top 20 palabras\n",
        "frecuencia_palabras.most_common(20)\n",
        "\n",
        "# ---\n",
        "# 游늷 Secci칩n 6: Visualizaci칩n de frecuencias\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "palabras, frecuencias = zip(*frecuencia_palabras.most_common(25))\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(palabras, frecuencias)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top 25 palabras m치s frecuentes\")\n",
        "plt.show()\n",
        "\n",
        "# ---\n",
        "# 游늷 Secci칩n 7: Topic Modeling con LDA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Unimos los tokens por documento\n",
        "documentos = [\" \".join(texto) for texto in textos_filtrados]\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
        "X = vectorizer.fit_transform(documentos)\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# Funci칩n para mostrar palabras top por tema\n",
        "def mostrar_top_palabras_por_tema(modelo, vectorizer, n_palabras=10):\n",
        "    palabras = vectorizer.get_feature_names_out()\n",
        "    for idx, tema in enumerate(modelo.components_):\n",
        "        print(f\"\\n游댳 Tema {idx + 1}:\")\n",
        "        top_palabras = [palabras[i] for i in tema.argsort()[:-n_palabras - 1:-1]]\n",
        "        print(\", \".join(top_palabras))\n",
        "\n",
        "mostrar_top_palabras_por_tema(lda, vectorizer)\n",
        "\n",
        "# ---\n",
        "# 游늷 Secci칩n 8: Visualizaci칩n interactiva de temas\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.sklearn.prepare(lda, X, vectorizer)"
      ],
      "metadata": {
        "id": "5KzrggkoxI49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdQtqAO_oEJ2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"IA.xlsx\")\n",
        "df.head()\n",
        "# Filtrar solo las filas con ese valor exacto en la columna \"Nombre de Intent\"\n",
        "df_filtrado = df[df[\"Nombre de Intent\"] == \"0.0. Enviar mensaje a LLM Default\"]\n",
        "print(df_filtrado.shape)  # Ver cu치ntas filas quedaron\n",
        "print(df_filtrado.head())  # Ver una muestra de las primeras filas\n",
        "textos_entrada = df_filtrado[\"Texto de Entrada\"].dropna().tolist()\n",
        "print(textos_entrada[:10])\n",
        "frases_irrelevantes = {\n",
        "    \"s칤\", \"ok\", \"vale\", \"gracias\", \"muchas gracias\", \"por favor\", \"hola\", \"buenos d칤as\",\n",
        "    \"buenas\", \"buenas tardes\", \"buenas noches\", \"de acuerdo\", \"est치 bien\", \"dale\", \"quiero ayuda\", \"por favor\"\n",
        "}\n",
        "def es_relevante(frase):\n",
        "    frase_limpia = frase.strip().lower()\n",
        "    # Filtrar frases demasiado cortas o vac칤as\n",
        "    if len(frase_limpia.split()) <= 2:\n",
        "        return False\n",
        "    # Filtrar saludos, cortes칤as, frases vac칤as\n",
        "    if frase_limpia in frases_irrelevantes:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "import re\n",
        "\n",
        "# 1. Eliminar entradas que contengan \"event detection\" (sin distinguir may칰sculas)\n",
        "textos_filtrados = [\n",
        "    texto for texto in textos_entrada\n",
        "    if \"event detection\" not in texto.lower()\n",
        "]\n",
        "\n",
        "# 2. Funci칩n para limpieza m칤nima (min칰sculas + sin signos)\n",
        "def limpiar_frase(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r'[^\\w\\s치칠칤칩칰칲침]', '', texto)  # elimina signos de puntuaci칩n pero mantiene letras con tildes y 침\n",
        "    return texto\n",
        "\n",
        "# 3. Aplicar limpieza\n",
        "frases_limpias = [limpiar_frase(texto) for texto in textos_filtrados]\n",
        "\n",
        "!pip install bertopic\n",
        "\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Crear modelo (multilingual sirve para espa침ol)\n",
        "modelo_topic = BERTopic(language=\"multilingual\", verbose=True)\n",
        "\n",
        "# Entrenar modelo\n",
        "temas, probabilidades = modelo_topic.fit_transform(frases_limpias)\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# Crear un clusterizador que agrupe mejor (menos temas)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=200, min_samples=1, metric='euclidean', prediction_data=True)\n",
        "\n",
        "# Entrenar el modelo con menos fragmentaci칩n\n",
        "modelo_topic = BERTopic(hdbscan_model=hdbscan_model, language=\"multilingual\", verbose=True)\n",
        "temas, probabilidades = modelo_topic.fit_transform(frases_limpias)\n",
        "\n",
        "df_temas = modelo_topic.get_topic_info()\n",
        "print(df_temas.head(10))  # Ver los 10 temas m치s frecuentes\n",
        "\n",
        "modelo_topic.get_topic(-1)\n",
        "\n",
        "# Por ejemplo, textos asignados al tema 1\n",
        "indices_tema_1 = [i for i, t in enumerate(temas) if t == 1]\n",
        "\n",
        "# Mostrar esos textos\n",
        "for i in indices_tema_1[:5]:  # solo los primeros 5\n",
        "    print(f\"- {frases_limpias[i]}\")\n",
        "modelo_topic.visualize_topics()\n",
        "\n",
        "modelo_topic.visualize_barchart(top_n_topics=42)\n",
        "\n",
        "modelo_topic_reducido = modelo_topic.reduce_topics(\n",
        "    frases_limpias,\n",
        "    nr_topics=20  # for example, reduce to 6 main topics\n",
        ")\n",
        "\n",
        "# Get the new topic assignments for the documents using the reduced model\n",
        "nuevo_temas, probabilidades = modelo_topic_reducido.transform(frases_limpias)\n",
        "\n",
        "modelo_topic_reducido.visualize_barchart(top_n_topics=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bertopic hdbscan openpyxl pandas\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Cargar el archivo Excel\n",
        "df = pd.read_excel(\"IA.xlsx\")\n",
        "\n",
        "# Filtrar por el intent del modelo LLM\n",
        "df_filtrado = df[df[\"Nombre de Intent\"] == \"0.0. Enviar mensaje a LLM Default\"]\n",
        "\n",
        "# Eliminar entradas vac칤as y las que contengan \"from_zendesk_full\"\n",
        "textos = df_filtrado[\"Texto de Entrada\"].dropna()\n",
        "textos = textos[~textos.str.contains(\"from_zendesk_full\", case=False)]\n",
        "\n",
        "# Convertir a lista\n",
        "frases = textos.tolist()\n",
        "\n",
        "# Filtrar frases muy cortas o irrelevantes\n",
        "frases_limpias = [\n",
        "    f for f in frases\n",
        "    if isinstance(f, str)\n",
        "    and len(f.split()) > 2\n",
        "    and not any(p in f.lower() for p in [\"gracias\", \"hola\", \"s칤\", \"ok\", \"por favor\", \"event\"])\n",
        "]\n",
        "\n",
        "\n",
        "# Crear clusterizador m치s estricto\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=200, min_samples=10, metric='euclidean', prediction_data=True)\n",
        "\n",
        "# Crear modelo BERTopic\n",
        "modelo_topic = BERTopic(hdbscan_model=hdbscan_model, language=\"multilingual\", verbose=True)\n",
        "\n",
        "# Entrenar modelo\n",
        "temas, probs = modelo_topic.fit_transform(frases_limpias)\n",
        "\n",
        "# Reducir n칰mero de temas (opcional: puedes ajustar `nr_topics`)\n",
        "modelo_topic_reducido = modelo_topic.reduce_topics(frases_limpias, nr_topics=20)\n",
        "\n",
        "# Obtener info de temas\n",
        "df_temas = modelo_topic_reducido.get_topic_info()\n",
        "\n",
        "# Eliminar temas irrelevantes (revisar manualmente con get_topic_info())\n",
        "temas_a_eliminar = []\n",
        "\n",
        "# Por ejemplo, elimina los que contengan palabras muy gen칠ricas\n",
        "palabras_irrelevantes = [\"gracias\", \"hola\", \"ok\", \"por\", \"favor\", \"s칤\"]\n",
        "for topic in df_temas[\"Topic\"]:\n",
        "    palabras_topico = [w[0] for w in modelo_topic_reducido.get_topic(topic)]\n",
        "    if any(pal in palabras_topico for pal in palabras_irrelevantes):\n",
        "        temas_a_eliminar.append(topic)\n",
        "\n",
        "# Filtrar\n",
        "df_temas_limpios = df_temas[~df_temas.Topic.isin(temas_a_eliminar)]\n",
        "\n",
        "# Ordenar por frecuencia\n",
        "df_temas_ordenados = df_temas_limpios.sort_values(by=\"Count\", ascending=False)\n",
        "\n",
        "# Exportar a Excel\n",
        "df_temas_ordenados.to_excel(\"temas_bot_limpios.xlsx\", index=False)\n",
        "\n",
        "# Vista previa\n",
        "print(df_temas_ordenados.head(10))\n",
        "for topic_id in df_temas[\"Topic\"]:\n",
        "    print(f\"Tema {topic_id}:\")\n",
        "    print(modelo_topic_reducido.get_topic(topic_id))\n",
        "    print(\"\\n\")\n",
        "\n",
        "etiquetas_manual = {\n",
        "    -1: \"Problemas generales con la cuenta\",\n",
        "     0: \"Promociones de giros gratuitos\",\n",
        "     1: \"Cambio de correo electr칩nico\",\n",
        "     2: \"Peticiones de ayuda gen칠ricas\",\n",
        "     3: \"Reclamaci칩n por giros no entregados\",\n",
        "     4: \"Problemas con bono de registro\",\n",
        "     5: \"Dudas sobre apuestas deportivas o casino\",\n",
        "     6: \"Preguntas sobre juegos y beneficios\",\n",
        "     7: \"Problemas para retirar dinero\",\n",
        "     8: \"Verificaci칩n de cuenta\",\n",
        "     9: \"D칩nde est치n mis giros / promociones\",\n",
        "    10: \"Solicitud para hablar con un agente\",\n",
        "    11: \"Consultas sobre promociones del d칤a\",\n",
        "    12: \"Bono de bienvenida / c칩mo usarlo\",\n",
        "    13: \"Promoci칩n de pron칩stico sin riesgo\",\n",
        "    14: \"Primer dep칩sito / recarga inicial\",\n",
        "    15: \"Reclamaciones por espera de 24-72h\",\n",
        "    16: \"Promoci칩n de cumplea침os\",\n",
        "    17: \"Promoci칩n D칤a del Padre\",\n",
        "    18: \"Frases poco informativas\"\n",
        "}\n",
        "\n",
        "df_temas = modelo_topic.get_topic_info()\n",
        "df_temas[\"Etiqueta Manual\"] = df_temas[\"Topic\"].map(etiquetas_manual)\n",
        "df_temas_ordenado = df_temas.sort_values(by=\"Count\", ascending=False)\n",
        "df_temas_ordenado.to_excel(\"temas_etiquetados.xlsx\", index=False)\n",
        "\n",
        "pip install transformers torch pysentimiento\n",
        "\n",
        "from pysentimiento import create_analyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Crear el analizador de sentimientos en espa침ol\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "\n",
        "# Lista de textos (usa las mismas frases que usaste para BERTopic)\n",
        "frases = frases_limpias  # lista de frases originales sin signos ni lematizar\n",
        "\n",
        "# Analizar los sentimientos\n",
        "resultados = [analyzer.predict(frase) for frase in frases]\n",
        "\n",
        "# Extraer sentimiento y score\n",
        "sentimientos = [r.output for r in resultados]\n",
        "probabilidades = [r.probas for r in resultados]\n",
        "\n",
        "# Crear DataFrame con los resultados\n",
        "df_sentimientos = pd.DataFrame({\n",
        "    \"Frase\": frases,\n",
        "    \"Sentimiento\": sentimientos,\n",
        "    \"Probabilidad\": [proba[max(proba, key=proba.get)] for proba in probabilidades]\n",
        "})\n",
        "\n",
        "# Ver resumen general\n",
        "resumen = df_sentimientos[\"Sentimiento\"].value_counts(normalize=True) * 100\n",
        "print(\"Distribuci칩n de sentimientos (%):\\n\")\n",
        "print(resumen.round(2))\n",
        "\n",
        "# Exportar si lo deseas\n",
        "df_sentimientos.to_excel(\"analisis_sentimientos.xlsx\", index=False)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar archivos\n",
        "df_temas = pd.read_excel(\"temas_bot_limpios.xlsx\")\n",
        "df_sentimientos = pd.read_excel(\"analisis_sentimientos.xlsx\")\n",
        "\n",
        "# Verificar contenido\n",
        "print(df_temas.columns)\n",
        "print(df_sentimientos.columns)\n",
        "\n",
        "df_temas_completo = modelo_topic_reducido.get_document_info(frases_limpias)\n",
        "df_temas_completo[[\"Document\", \"Topic\"]].rename(columns={\"Document\": \"Frase\", \"Topic\": \"TemaReducido\"}).to_excel(\"temas_bot_limpios_completo.xlsx\", index=False)\n",
        "\n",
        "df_temas = pd.read_excel(\"temas_bot_limpios_completo.xlsx\")\n",
        "df_sentimientos = pd.read_excel(\"analisis_sentimientos.xlsx\")\n",
        "\n",
        "# Unir por la frase\n",
        "df_completo = pd.merge(df_temas, df_sentimientos, on=\"Frase\", how=\"inner\")\n",
        "\n",
        "# Agrupar por TemaReducido y Sentimiento\n",
        "resumen = df_completo.groupby([\"TemaReducido\", \"Sentimiento\"]).size().unstack(fill_value=0)\n",
        "\n",
        "# Calcular porcentaje por tema\n",
        "resumen_pct = resumen.div(resumen.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Ordenar por porcentaje de negativos\n",
        "resumen_ordenado = resumen_pct.sort_values(by=\"NEG\", ascending=False)\n",
        "\n",
        "print(resumen_ordenado.round(2))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Graficar los primeros 10 temas con m치s negatividad\n",
        "resumen_ordenado.head(10).plot(\n",
        "    kind='barh',\n",
        "    stacked=True,\n",
        "    color=[\"red\", \"gray\", \"green\"],\n",
        "    figsize=(10, 6)\n",
        ")\n",
        "plt.xlabel(\"Porcentaje\")\n",
        "plt.ylabel(\"Tema Reducido\")\n",
        "plt.title(\"Distribuci칩n de Sentimientos por Tema Reducido\")\n",
        "plt.legend(title=\"Sentimiento\", loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ApXdKngxqGms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VCWRSELuqY00"
      }
    }
  ]
}